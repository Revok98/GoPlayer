{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, LocallyConnected2D, SeparableConv2D\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 41563 examples\n"
     ]
    }
   ],
   "source": [
    "def get_raw_data_go(filename):\n",
    "    ''' Returns the set of samples from the local file or download it if it does not exists'''\n",
    "    import gzip, os.path\n",
    "    import json\n",
    "\n",
    "    raw_samples_file = filename\n",
    "\n",
    "    if not os.path.isfile(raw_samples_file):\n",
    "        print(\"File\", raw_samples_file, \"not found, I am downloading it...\", end=\"\")\n",
    "        import urllib.request \n",
    "        urllib.request.urlretrieve(\"https://www.labri.fr/perso/lsimon/ia-inge2/\"+filename, filename)\n",
    "        print(\" Done\")\n",
    "\n",
    "    with gzip.open(filename) as fz:\n",
    "        data = json.loads(fz.read().decode(\"utf-8\"))\n",
    "    return data\n",
    "\n",
    "data = get_raw_data_go(\"samples-9x9.json.gz\")\n",
    "print(\"We have\", len(data),\"examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41563, 9, 9, 2)\n",
      "(41563,)\n",
      "(665008, 9, 9, 2)\n",
      "(665008,)\n"
     ]
    }
   ],
   "source": [
    "def name_to_coord(s):\n",
    "    assert s != \"PASS\"\n",
    "    indexLetters = {'A':0, 'B':1, 'C':2, 'D':3, 'E':4, 'F':5, 'G':6, 'H':7, 'J':8}\n",
    "\n",
    "    col = indexLetters[s[0]]\n",
    "    lin = int(s[1:]) - 1\n",
    "    return col, lin\n",
    "\n",
    "\n",
    "def stones_to_tensor(black_stones, white_stones) :\n",
    "    # insérer les pions noirs\n",
    "    B = np.zeros((9, 9))\n",
    "    for p in black_stones :\n",
    "        i, j = name_to_coord(p)\n",
    "        B[i, j] = 1\n",
    "    # insérer les pions blancs\n",
    "    W = np.zeros((9, 9))\n",
    "    for p in white_stones :\n",
    "        i, j = name_to_coord(p)\n",
    "        W[i, j] = 1\n",
    "        \n",
    "    return np.array([B,W]).reshape(9,9,2)\n",
    "\n",
    "\n",
    "#Vous prendrez en entrée data[i][\"black_stones\"] et data[i][\"white_stones\"]\n",
    "X_tmp = np.array([stones_to_tensor(stones[\"black_stones\"], stones[\"white_stones\"]) for stones in data])\n",
    "#Vous devrez prédire simplement data[i][\"black_wins\"]/data[i][\"rollouts\"]\n",
    "y_tmp = np.array([d[\"black_wins\"] / d[\"rollouts\"] for d in data])\n",
    "\n",
    "print(X_tmp.shape)\n",
    "print(y_tmp.shape)\n",
    "\n",
    "# ajouter les symétries et les rotations\n",
    "# 12  24  43  31\n",
    "# 34  13  21  42\n",
    "# \n",
    "# 34  13  21  42\n",
    "# 12  24  43  31\n",
    "\n",
    "X = list()\n",
    "y = list()\n",
    "\n",
    "for i in range(len(X_tmp)) :\n",
    "    new = list()\n",
    "    new.append(X_tmp[i].reshape(2,9,9))\n",
    "    new.append(np.array([np.flipud(new[-1][0]), np.flipud(new[-1][1])]))\n",
    "    new.append(np.array([np.rot90(new[-2][0]), np.rot90(new[-2][1])]))\n",
    "    new.append(np.array([np.flipud(new[-1][0]), np.flipud(new[-1][1])]))\n",
    "    new.append(np.array([np.rot90(new[-2][0]), np.rot90(new[-2][1])]))\n",
    "    new.append(np.array([np.flipud(new[-1][0]), np.flipud(new[-1][1])]))\n",
    "    new.append(np.array([np.rot90(new[-2][0]), np.rot90(new[-2][1])]))\n",
    "    new.append(np.array([np.flipud(new[-1][0]), np.flipud(new[-1][1])]))   \n",
    "    X += [r.reshape(9,9,2) for r in new]\n",
    "    y += [y_tmp[i]] * 8\n",
    "    # ajouter en inversant noirs/blancs \n",
    "    X += [np.array([r[1], r[0]]).reshape(9,9,2) for r in new]\n",
    "    y += [1-y_tmp[i]] * 8\n",
    "\n",
    "       \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 9, 9, 32)          608       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 9, 9, 32)          128       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2592)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2592)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 648)               1680264   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 648)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 324)               210276    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 162)               52650     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 84)                13692     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 42)                3570      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 21)                903       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 22        \n",
      "=================================================================\n",
      "Total params: 1,962,113\n",
      "Trainable params: 1,962,049\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def model_julien():\n",
    "    λ = optimizers.schedules.InverseTimeDecay( # learning rate\n",
    "        initial_learning_rate=1e-3,\n",
    "        decay_steps=15000,\n",
    "        decay_rate=0.5)\n",
    "\n",
    "    α = 0.4 # dropout rate\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(9,9,2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(rate=α))\n",
    "    model.add(Dense(648, activation='relu'))\n",
    "    model.add(Dropout(rate=α))\n",
    "    model.add(Dense(324, activation='relu'))\n",
    "    model.add(Dense(162, activation='relu'))\n",
    "    model.add(Dense(84, activation='relu'))\n",
    "    model.add(Dense(42, activation='relu'))\n",
    "    model.add(Dense(21, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizers.RMSprop(λ), loss='mean_absolute_error')\n",
    "\n",
    "    # optimizer : optimizers.RMSprop(λ) ; optimizers.Adam(learning_rate=λ) ; optimizers.SGD(learning_rate=λ)\n",
    "    # loss : binary_crossentropy ; mean_squared_error ; mean_absolute_error\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def tuned_model(): #Tuned with Keras Tuner\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=5, activation='relu', input_shape=(9,9,2), padding =\"same\"))\n",
    "    model.add(Conv2D(256, kernel_size=5, activation='relu', padding =\"same\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax')) \n",
    "    opt= optimizers.Adam()\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                optimizer = opt,\n",
    "                metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = model_julien()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "# ==================================================================================\n",
    "class History(Callback):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.history = {}\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):       \n",
    "        for k,v in logs.items():\n",
    "            if not k in self.history: self.history[k]=[]\n",
    "            self.history[k].append(v)\n",
    "        print(\".\",end=\"\")\n",
    "history=[History()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Epoch 1/50\n",
      "2228/2228 [==============================] - 58s 26ms/step - loss: 0.1683 - val_loss: 0.1356\n",
      "Epoch 2/50\n",
      "2228/2228 [==============================] - 53s 24ms/step - loss: 0.1446 - val_loss: 0.1272\n",
      "Epoch 3/50\n",
      "2228/2228 [==============================] - 53s 24ms/step - loss: 0.1387 - val_loss: 0.1256\n",
      "Epoch 4/50\n",
      "2228/2228 [==============================] - 55s 25ms/step - loss: 0.1344 - val_loss: 0.1254\n",
      "Epoch 5/50\n",
      "2228/2228 [==============================] - 58s 26ms/step - loss: 0.1317 - val_loss: 0.1206\n",
      "Epoch 6/50\n",
      "2228/2228 [==============================] - 54s 24ms/step - loss: 0.1298 - val_loss: 0.1171\n",
      "Epoch 7/50\n",
      "2228/2228 [==============================] - 60s 27ms/step - loss: 0.1281 - val_loss: 0.1159\n",
      "Epoch 8/50\n",
      "2228/2228 [==============================] - 60s 27ms/step - loss: 0.1262 - val_loss: 0.1173\n",
      "Epoch 9/50\n",
      "2228/2228 [==============================] - 57s 26ms/step - loss: 0.1251 - val_loss: 0.1162\n",
      "Epoch 10/50\n",
      "2228/2228 [==============================] - 56s 25ms/step - loss: 0.1244 - val_loss: 0.1158\n",
      "Epoch 11/50\n",
      "2228/2228 [==============================] - 62s 28ms/step - loss: 0.1231 - val_loss: 0.1151\n",
      "Epoch 12/50\n",
      "2228/2228 [==============================] - 64s 29ms/step - loss: 0.1223 - val_loss: 0.1136\n",
      "Epoch 13/50\n",
      "2228/2228 [==============================] - 68s 30ms/step - loss: 0.1210 - val_loss: 0.1135\n",
      "Epoch 14/50\n",
      "2228/2228 [==============================] - 72s 32ms/step - loss: 0.1206 - val_loss: 0.1125\n",
      "Epoch 15/50\n",
      "2228/2228 [==============================] - 72s 32ms/step - loss: 0.1201 - val_loss: 0.1127\n",
      "Epoch 16/50\n",
      "2228/2228 [==============================] - 75s 33ms/step - loss: 0.1196 - val_loss: 0.1120\n",
      "Epoch 17/50\n",
      "2228/2228 [==============================] - 75s 33ms/step - loss: 0.1188 - val_loss: 0.1114\n",
      "Epoch 18/50\n",
      "2228/2228 [==============================] - 69s 31ms/step - loss: 0.1181 - val_loss: 0.1106\n",
      "Epoch 19/50\n",
      "2228/2228 [==============================] - 57s 26ms/step - loss: 0.1177 - val_loss: 0.1113\n",
      "Epoch 20/50\n",
      "2228/2228 [==============================] - 57s 26ms/step - loss: 0.1172 - val_loss: 0.1115\n",
      "Epoch 21/50\n",
      "2228/2228 [==============================] - 57s 26ms/step - loss: 0.1167 - val_loss: 0.1105\n",
      "Epoch 22/50\n",
      "2228/2228 [==============================] - 58s 26ms/step - loss: 0.1160 - val_loss: 0.1095\n",
      "Epoch 23/50\n",
      "2228/2228 [==============================] - 57s 26ms/step - loss: 0.1155 - val_loss: 0.1092\n",
      "Epoch 24/50\n",
      "2228/2228 [==============================] - 59s 27ms/step - loss: 0.1153 - val_loss: 0.1088\n",
      "Epoch 25/50\n",
      "2228/2228 [==============================] - 59s 26ms/step - loss: 0.1147 - val_loss: 0.1085\n",
      "Epoch 26/50\n",
      "2228/2228 [==============================] - 58s 26ms/step - loss: 0.1146 - val_loss: 0.1086\n",
      "Epoch 27/50\n",
      "2228/2228 [==============================] - 60s 27ms/step - loss: 0.1142 - val_loss: 0.1083\n",
      "Epoch 28/50\n",
      "2228/2228 [==============================] - 59s 27ms/step - loss: 0.1134 - val_loss: 0.1094\n",
      "Epoch 29/50\n",
      "2228/2228 [==============================] - 60s 27ms/step - loss: 0.1135 - val_loss: 0.1081\n",
      "Epoch 30/50\n",
      "2228/2228 [==============================] - 63s 28ms/step - loss: 0.1131 - val_loss: 0.1077\n",
      "Epoch 31/50\n",
      "2228/2228 [==============================] - 65s 29ms/step - loss: 0.1130 - val_loss: 0.1077\n",
      "Epoch 32/50\n",
      "2228/2228 [==============================] - 62s 28ms/step - loss: 0.1123 - val_loss: 0.1078\n",
      "Epoch 33/50\n",
      "2228/2228 [==============================] - 60s 27ms/step - loss: 0.1124 - val_loss: 0.1082\n",
      "Epoch 34/50\n",
      "2228/2228 [==============================] - 63s 28ms/step - loss: 0.1120 - val_loss: 0.1078\n",
      "Epoch 35/50\n",
      "2228/2228 [==============================] - 61s 28ms/step - loss: 0.1115 - val_loss: 0.1085\n",
      "Epoch 36/50\n",
      "2228/2228 [==============================] - 62s 28ms/step - loss: 0.1113 - val_loss: 0.1077\n",
      "Epoch 37/50\n",
      "2228/2228 [==============================] - 63s 28ms/step - loss: 0.1113 - val_loss: 0.1072\n",
      "Epoch 38/50\n",
      "2228/2228 [==============================] - 60s 27ms/step - loss: 0.1109 - val_loss: 0.1069\n",
      "Epoch 39/50\n",
      "2228/2228 [==============================] - 59s 27ms/step - loss: 0.1109 - val_loss: 0.1073\n",
      "Epoch 40/50\n",
      "2228/2228 [==============================] - 59s 27ms/step - loss: 0.1103 - val_loss: 0.1072\n",
      "Epoch 41/50\n",
      "2228/2228 [==============================] - 59s 27ms/step - loss: 0.1106 - val_loss: 0.1067\n",
      "Epoch 42/50\n",
      "2228/2228 [==============================] - 59s 27ms/step - loss: 0.1100 - val_loss: 0.1067\n",
      "Epoch 43/50\n",
      "2228/2228 [==============================] - 59s 27ms/step - loss: 0.1099 - val_loss: 0.1073\n",
      "Epoch 44/50\n",
      "2228/2228 [==============================] - 60s 27ms/step - loss: 0.1098 - val_loss: 0.1074\n",
      "Epoch 45/50\n",
      "2228/2228 [==============================] - 60s 27ms/step - loss: 0.1094 - val_loss: 0.1072\n",
      "Epoch 46/50\n",
      "2228/2228 [==============================] - 71s 32ms/step - loss: 0.1097 - val_loss: 0.1069\n",
      "Epoch 47/50\n",
      "2228/2228 [==============================] - 70s 31ms/step - loss: 0.1092 - val_loss: 0.1067\n",
      "Epoch 48/50\n",
      "2228/2228 [==============================] - 67s 30ms/step - loss: 0.1087 - val_loss: 0.1067\n",
      "Epoch 49/50\n",
      "2228/2228 [==============================] - 68s 31ms/step - loss: 0.1086 - val_loss: 0.1064\n",
      "Epoch 50/50\n",
      "2228/2228 [==============================] - 71s 32ms/step - loss: 0.1086 - val_loss: 0.1064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4f06ebc460>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#es_callback = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# hyperparamètres\n",
    "epochs = 50\n",
    "batch_size = 200\n",
    "\n",
    "# training\n",
    "print(\"Training ...\")\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training :\n",
      "Test loss: 0.10642054677009583\n",
      "Verification on train : \n",
      "expected :  [0.51 0.89 0.52 0.83 0.42 0.37 1.   0.   0.09 0.82 0.63 0.   0.21 0.41]\n",
      "got      :  [0.46 0.72 0.47 0.76 0.35 0.37 0.87 0.09 0.21 0.87 0.48 0.   0.22 0.48]\n",
      "Verification on test : \n",
      "expected :  [0.68 1.   0.23 0.   0.01 0.   0.   1.   1.   0.16 0.08 0.02 0.4  0.21]\n",
      "got      :  [0.4  1.   0.1  0.   0.   0.   0.11 1.   1.   0.02 0.1  0.05 0.5  0.46]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5adeb776812d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Model accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Loss (on training data)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "print(\"After training :\")\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score)\n",
    "\n",
    "nb_to_test = 14\n",
    "\n",
    "print(\"Verification on train : \")\n",
    "print(\"expected : \", y_train[0:nb_to_test])\n",
    "print(\"got      : \", model.predict(X_train[0:nb_to_test]).reshape(1,nb_to_test)[0].round(2))\n",
    "\n",
    "print(\"Verification on test : \")\n",
    "print(\"expected : \", y_test[0:nb_to_test])\n",
    "print(\"got      : \", model.predict(X_test[0:nb_to_test]).reshape(1,nb_to_test)[0].round(2))\n",
    "\n",
    "# Model accuracy\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss (on training data)')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Loss', 'Loss (sur validation)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.reshape(1, y_test.shape[0])[0]\n",
    "    \n",
    "def print_repartition(y_pred, y_test) :\n",
    "    delta = abs(y_pred - y_test)\n",
    "    stat = [0] * 20\n",
    "    for val in delta :\n",
    "        for i in range(20) :\n",
    "            if val <= 0.05*(i+1) :\n",
    "                stat[i] += 1\n",
    "                break\n",
    "    for i in range(20) :\n",
    "        print(\"val <= \" + str(np.round(0.05*(i+1),3)) + \" : \", 100*stat[i]/len(y_test), \"%\")\n",
    "        \n",
    "print_repartition(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
