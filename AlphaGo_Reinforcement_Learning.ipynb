{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from tensorflow import keras\n",
    "import Goban\n",
    "import mctsPlayerNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay-Memory\n",
    "\n",
    "On définit ici une classe permettant de stocker les actions effectuées par l'agent ainsi que les rewards obtenues, afin d'entrainer le réseau de neurones par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction d'augmentation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symetries_rotations(x):\n",
    "    # input_shape = (9,9,k)\n",
    "    new = list()\n",
    "    new.append(x)\n",
    "    new.append(np.flipud(new[-1]))\n",
    "    new.append(np.rot90(new[-2]))\n",
    "    new.append(np.flipud(new[-1]))\n",
    "    new.append(np.rot90(new[-2]))\n",
    "    new.append(np.flipud(new[-1]))\n",
    "    new.append(np.rot90(new[-2]))\n",
    "    new.append(np.flipud(new[-1]))\n",
    "    return new\n",
    "\n",
    "def data_augmentation(states, actions, next_states, rewards):\n",
    "    augmented_states = list()\n",
    "    augmented_actions = list()\n",
    "    augmented_next_states = list()\n",
    "    augmented_rewards = list()\n",
    "    \n",
    "    for i in range(len(states)):\n",
    "        augmented_states += symetries_rotations(states[i])\n",
    "        augmented_next_states += symetries_rotations(next_states[i])\n",
    "        augmented_actions += [actions[i]] * 8\n",
    "        augmented_rewards += [rewards[i]] * 8\n",
    "        \n",
    "    return augmented_states, augmented_actions, augmented_next_states, augmented_rewards      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction qui convertit un Goban.Board en numpy.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def board_encoding(board, liberties=0):\n",
    "    boards = np.zeros((9,9,3+liberties))\n",
    "    for x in range(9):\n",
    "        for y in range(9):\n",
    "            c = board._board[board.flatten((x,y))]\n",
    "            if c == board._BLACK:\n",
    "                boards[x,y,0] = 1\n",
    "            elif c == board._WHITE:\n",
    "                boards[x,y,1] = 1\n",
    "            if liberties > 0:\n",
    "                l = min(board._stringLiberties[board.flatten((x,y))], liberties-1)\n",
    "                boards[x,y,l+2] = 1\n",
    "    if board._nextPlayer != board._BLACK:\n",
    "        boards[:,:,-1] = 1\n",
    "    return boards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction qui réalise une partie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play():\n",
    "    player1 = mctsPlayerNN.myPlayer()\n",
    "    player2 = mctsPlayerNN.myPlayer()\n",
    "\n",
    "    player1.newGame(Goban.Board._BLACK)\n",
    "    player2.newGame(Goban.Board._WHITE)\n",
    "    players = [player1, player2]\n",
    "\n",
    "    b = Goban.Board()\n",
    "    nextplayer = 0\n",
    "    nextplayercolor = Goban.Board._BLACK\n",
    "\n",
    "    states = list()\n",
    "    next_states = list()\n",
    "    actions = list()\n",
    "\n",
    "    while not b.is_game_over():\n",
    "        # save the board as state\n",
    "        states.append(board_encoding(b, liberties=3))\n",
    "        legals = b.legal_moves()\n",
    "        otherplayer = (nextplayer + 1) % 2\n",
    "        othercolor = Goban.Board.flip(nextplayercolor)\n",
    "        move = players[nextplayer].getPlayerMove()\n",
    "        # save the move as chosen action\n",
    "        actions.append(Goban.Board.name_to_flat(move))\n",
    "        if not Goban.Board.name_to_flat(move) in legals:\n",
    "            # illegal move\n",
    "            return None, None, None, None\n",
    "        b.push(Goban.Board.name_to_flat(move))\n",
    "        next_states.append(board_encoding(b, liberties=3))\n",
    "        players[otherplayer].playOpponentMove(move)\n",
    "        nextplayer = otherplayer\n",
    "        nextplayercolor = othercolor\n",
    "\n",
    "    result = b.result()\n",
    "    if result == \"1-0\": winner = 1\n",
    "    elif result == \"0-1\": winner = 0\n",
    "    else: winner = -1\n",
    "\n",
    "    # give rewards\n",
    "    rewards = [(-1)**(n+winner) for n in range(len(actions))] if winner != -1 else [0]*len(actions)\n",
    "    \n",
    "    return states, actions, next_states, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage du Réseau de Neurones\n",
    "\n",
    "On sélectionne un batch de manière aléatoire depuis la mémoire, puis on entraine le réseau à prédire avec les rewards associés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "model_priors = keras.models.load_model('model/model_priors.h5')\n",
    "model_values = keras.models.load_model('model/model_values.h5')\n",
    "\n",
    "def optimize_model(memory):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    train_state = list()\n",
    "    train_priors = list()\n",
    "    train_values = list()\n",
    "    for state, action, next_state, reward in batch:\n",
    "        priors = model_priors.predict(np.expand_dims(state, axis=0))[0]\n",
    "        next_priors = model_priors.predict(np.expand_dims(next_state, axis=0))[0]\n",
    "        priors[action] = reward + GAMMA * np.amax(next_priors)\n",
    "        train_state.append(state)\n",
    "        train_priors.append(priors)\n",
    "        train_values.append(reward)\n",
    "        \n",
    "    train_state = np.array(train_state)\n",
    "    train_priors = np.array(train_priors)\n",
    "    train_values = np.array(train_values)\n",
    "        \n",
    "    model_priors.fit(train_state, train_priors, epochs=1, verbose=0)\n",
    "    model_values.fit(train_state, train_values, epochs=1, verbose=0)\n",
    "    # les modèles sont sauvés pour être utilisés par les joueurs suivants\n",
    "    model_priors.save('model/model_priors.h5')\n",
    "    model_values.save('model/model_values.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boucle d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparamètres\n",
    "N_EPISODES = 10\n",
    "memory = ReplayMemory(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: playing... done (42 states)\n",
      "Episode 0: data augmentation... done (336 states)\n",
      "Episode 0: optimizing models...\n",
      "Episode 0 complete! (182s)\n",
      "-------------------------------------\n",
      "Episode 1: playing... done (39 states)\n",
      "Episode 1: data augmentation... done (312 states)\n",
      "Episode 1: optimizing models...\n",
      "Episode 1 complete! (170s)\n",
      "-------------------------------------\n",
      "Episode 2: playing... error! (illegal move)\n",
      "Episode 2: data augmentation... "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b9eaba5389d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# augmenter les données\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Episode {i}: data augmentation...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"done ({len(states)} states)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-75c7dd62a450>\u001b[0m in \u001b[0;36mdata_augmentation\u001b[0;34m(states, actions, next_states, rewards)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0maugmented_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0maugmented_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msymetries_rotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0maugmented_next_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msymetries_rotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "for i in range(N_EPISODES):\n",
    "    start = time.time()\n",
    "    # faire une partie entre deux joueurs\n",
    "    print(f\"Episode {i}: playing...\", end=\" \")\n",
    "    states, actions, next_states, rewards = play()\n",
    "    if states is None: \n",
    "        print(\"error! (illegal move)\")\n",
    "        continue\n",
    "    else: \n",
    "        print(f\"done ({len(states)} states)\")\n",
    "    \n",
    "    # augmenter les données\n",
    "    print(f\"Episode {i}: data augmentation...\", end=\" \")\n",
    "    states, actions, next_states, rewards = data_augmentation(states, actions, next_states, rewards)\n",
    "    print(f\"done ({len(states)} states)\")\n",
    "    \n",
    "    # stocker en mémoire les plateaux/actions/récompenses obtenus lors de la partie\n",
    "    for state, action, next_state, reward in zip(states, actions, next_states, rewards):\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        \n",
    "    # mettre à jour les réseaux de neurones\n",
    "    print(f\"Episode {i}: optimizing models...\")\n",
    "    optimize_model(memory)\n",
    "    \n",
    "    print(f\"Episode {i} complete! ({round(time.time() - start)}s)\")\n",
    "    print(\"-------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
