{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, LocallyConnected2D, SeparableConv2D\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras import activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_data import create_all_x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 parties rejetées par le goban, reste 2474 parties\n",
      "(15833, 9, 9, 49)\n",
      "(15833, 82)\n"
     ]
    }
   ],
   "source": [
    "X, y = create_all_x_y()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 9, 9, 32)          14144     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 9, 9, 32)          128       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2592)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2592)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 648)               1680264   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 648)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 324)               210276    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 162)               52650     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 82)                13366     \n",
      "=================================================================\n",
      "Total params: 1,970,828\n",
      "Trainable params: 1,970,764\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 9, 9, 32)          14144     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 9, 9, 32)          128       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2592)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2592)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 648)               1680264   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 648)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 324)               210276    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 162)               52650     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 82)                13366     \n",
      "=================================================================\n",
      "Total params: 1,970,828\n",
      "Trainable params: 1,970,764\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def model_julien():\n",
    "    λ = optimizers.schedules.InverseTimeDecay( # learning rate\n",
    "        initial_learning_rate=1e-3,\n",
    "        decay_steps=15000,\n",
    "        decay_rate=0.5)\n",
    "\n",
    "    α = 0.4 # dropout rate\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(9,9,49), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(rate=α))\n",
    "    model.add(Dense(648, activation='relu'))\n",
    "    model.add(Dropout(rate=α))\n",
    "    model.add(Dense(324, activation='relu'))\n",
    "    model.add(Dense(162, activation='relu'))\n",
    "    model.add(Dense(82, activation='relu'))\n",
    "    \n",
    "\n",
    "    model.compile(optimizers.RMSprop(λ), loss='mean_absolute_error')\n",
    "\n",
    "    # optimizer : optimizers.RMSprop(λ) ; optimizers.Adam(learning_rate=λ) ; optimizers.SGD(learning_rate=λ)\n",
    "    # loss : binary_crossentropy ; mean_squared_error ; mean_absolute_error\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=5, activation='relu', input_shape=X[0].shape, padding =\"same\"))\n",
    "model.add(Conv2D(256, kernel_size=5, activation='relu', padding =\"same\"))\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', padding =\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(82, activation='elu')) \n",
    "opt= optimizers.Adam()\n",
    "model.compile(loss='mae',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Epoch 1/50\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.2460 - val_loss: 0.2663\n",
      "Epoch 2/50\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.2160 - val_loss: 0.2489\n",
      "Epoch 3/50\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.2128 - val_loss: 0.2255\n",
      "Epoch 4/50\n",
      "248/248 [==============================] - 4s 17ms/step - loss: 0.2109 - val_loss: 0.2203\n",
      "Epoch 5/50\n",
      "248/248 [==============================] - 4s 17ms/step - loss: 0.2098 - val_loss: 0.2273\n",
      "Epoch 6/50\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.2087 - val_loss: 0.2218\n",
      "Epoch 7/50\n",
      "248/248 [==============================] - 5s 18ms/step - loss: 0.2078 - val_loss: 0.2314\n",
      "Epoch 8/50\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 0.2070 - val_loss: 0.2161\n",
      "Epoch 9/50\n",
      "248/248 [==============================] - 5s 21ms/step - loss: 0.2060 - val_loss: 0.2187\n",
      "Epoch 10/50\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.2056 - val_loss: 0.2169\n",
      "Epoch 11/50\n",
      "248/248 [==============================] - 4s 17ms/step - loss: 0.2047 - val_loss: 0.2111\n",
      "Epoch 12/50\n",
      "248/248 [==============================] - 4s 17ms/step - loss: 0.2045 - val_loss: 0.2176\n",
      "Epoch 13/50\n",
      "248/248 [==============================] - 4s 18ms/step - loss: 0.2039 - val_loss: 0.2137\n",
      "Epoch 14/50\n",
      "248/248 [==============================] - 4s 18ms/step - loss: 0.2034 - val_loss: 0.2150\n",
      "Epoch 15/50\n",
      "248/248 [==============================] - 4s 18ms/step - loss: 0.2031 - val_loss: 0.2148\n",
      "Epoch 16/50\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.2027 - val_loss: 0.2112\n",
      "Epoch 17/50\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.2022 - val_loss: 0.2155\n",
      "Epoch 18/50\n",
      "248/248 [==============================] - 4s 17ms/step - loss: 0.2018 - val_loss: 0.2171\n",
      "Epoch 19/50\n",
      "248/248 [==============================] - 4s 17ms/step - loss: 0.2014 - val_loss: 0.2095\n",
      "Epoch 20/50\n",
      "248/248 [==============================] - 4s 16ms/step - loss: 0.1978 - val_loss: 0.2088\n",
      "Epoch 21/50\n",
      "248/248 [==============================] - 4s 17ms/step - loss: 0.1975 - val_loss: 0.2117\n",
      "Epoch 22/50\n",
      "248/248 [==============================] - 4s 17ms/step - loss: 0.1969 - val_loss: 0.2096\n",
      "Epoch 23/50\n",
      "248/248 [==============================] - 4s 17ms/step - loss: 0.1968 - val_loss: 0.2113\n",
      "Epoch 24/50\n",
      "248/248 [==============================] - 4s 17ms/step - loss: 0.1967 - val_loss: 0.2076\n",
      "Epoch 25/50\n",
      "248/248 [==============================] - 4s 18ms/step - loss: 0.1965 - val_loss: 0.2095\n",
      "Epoch 26/50\n",
      "248/248 [==============================] - 5s 18ms/step - loss: 0.1959 - val_loss: 0.2083\n",
      "Epoch 27/50\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.1959 - val_loss: 0.2125\n",
      "Epoch 28/50\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.1955 - val_loss: 0.2090\n",
      "Epoch 29/50\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 0.1951 - val_loss: 0.2085\n",
      "Epoch 30/50\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 0.1940 - val_loss: 0.2063\n",
      "Epoch 31/50\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 0.1913 - val_loss: 0.2040\n",
      "Epoch 32/50\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.1913 - val_loss: 0.2042\n",
      "Epoch 33/50\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 0.1907 - val_loss: 0.2072\n",
      "Epoch 34/50\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 0.1907 - val_loss: 0.2051\n",
      "Epoch 35/50\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.1904 - val_loss: 0.2040\n",
      "Epoch 36/50\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.1902 - val_loss: 0.2049\n",
      "Epoch 37/50\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.1899 - val_loss: 0.2040\n",
      "Epoch 38/50\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.1897 - val_loss: 0.2066\n",
      "Epoch 39/50\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 0.1895 - val_loss: 0.2024\n",
      "Epoch 40/50\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 0.1891 - val_loss: 0.2029\n",
      "Epoch 41/50\n",
      "248/248 [==============================] - 5s 21ms/step - loss: 0.1890 - val_loss: 0.2043\n",
      "Epoch 42/50\n",
      "248/248 [==============================] - 5s 18ms/step - loss: 0.1884 - val_loss: 0.2036\n",
      "Epoch 43/50\n",
      "248/248 [==============================] - 4s 17ms/step - loss: 0.1884 - val_loss: 0.2022\n",
      "Epoch 44/50\n",
      "248/248 [==============================] - 4s 17ms/step - loss: 0.1883 - val_loss: 0.2021\n",
      "Epoch 45/50\n",
      "248/248 [==============================] - 5s 18ms/step - loss: 0.1878 - val_loss: 0.2033\n",
      "Epoch 46/50\n",
      "248/248 [==============================] - 5s 21ms/step - loss: 0.1877 - val_loss: 0.2040\n",
      "Epoch 47/50\n",
      "248/248 [==============================] - 5s 21ms/step - loss: 0.1876 - val_loss: 0.2050\n",
      "Epoch 48/50\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 0.1873 - val_loss: 0.2021\n",
      "Epoch 49/50\n",
      "248/248 [==============================] - 5s 19ms/step - loss: 0.1869 - val_loss: 0.2050\n",
      "Epoch 50/50\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 0.1867 - val_loss: 0.2039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff43ca4e1f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparamètres\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "# training\n",
    "print(\"Training ...\")\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5228609  0.55699044 0.5362773  0.48673797 0.5731567  0.58022106\n",
      " 0.44858414 0.53510237 0.44719613 0.58502877 0.55413914 0.5147674\n",
      " 0.5453913  0.5646487  0.49700922 0.5359767  0.52565235 0.6096695\n",
      " 0.5047757  0.5869566  0.5456616  0.5773529  0.5962179  0.49978694\n",
      " 0.5577977  0.4602521  0.48288396 0.5676175  0.52821857 0.5870718\n",
      " 0.60353786 0.46679008 0.52251834 0.4387975  0.44744998 0.5341391\n",
      " 0.5221214  0.41564643 0.5767388  0.5387567  0.5859307  0.5048639\n",
      " 0.5151499  0.5288731  0.55381525 0.46310264 0.531793   0.57807016\n",
      " 0.5512063  0.5834061  0.5396954  0.52075064 0.5956475  0.5469848\n",
      " 0.5789376  0.6177249  0.50771105 0.6271464  0.4595841  0.5756392\n",
      " 0.5680325  0.5585363  0.46658286 0.57207024 0.5117404  0.51146805\n",
      " 0.5826603  0.50609857 0.61082166 0.5373887  0.5735155  0.5950719\n",
      " 0.5076919  0.5194654  0.50452113 0.58208615 0.5051     0.625646\n",
      " 0.6499548  0.5460365  0.5223807  0.47065794]\n"
     ]
    }
   ],
   "source": [
    "print(proba[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
